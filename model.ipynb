{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import collections\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.load('word2vec.npy')\n",
    "word2index = dict()\n",
    "with open('word_lst.txt') as file:\n",
    "    for counter, line in enumerate(file):\n",
    "        word = line.strip()\n",
    "        word2index[word] = counter\n",
    "\n",
    "def word2vec(word):\n",
    "    idx = word2index.get(word)\n",
    "    if idx is None:\n",
    "        return np.zeros(200)\n",
    "    return M[idx]\n",
    "\n",
    "        \n",
    "def get_vec(word):\n",
    "    try:\n",
    "        retval = M[word2index[word]]\n",
    "    except KeyError:\n",
    "        retval = np.zeros(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(filename):\n",
    "    \n",
    "    start_tag = '<S>'\n",
    "    end_tag = '</S>'\n",
    "    \n",
    "    sentence_lst = [] # Sentence is a list word which is list of candidate roots\n",
    "    sentence_correct_lst = []\n",
    "    with open(filename) as file:\n",
    "        start_parsing = False\n",
    "        for line in file:\n",
    "            if start_parsing:\n",
    "                if line.startswith(end_tag):\n",
    "                    start_parsing = False\n",
    "                    sentence_lst.append(sentence)\n",
    "                    sentence_correct_lst.append(sentence_correct)\n",
    "                else:\n",
    "                    root_set = set()\n",
    "                    candidate_lst = line.split()[1:]\n",
    "                    for parse in candidate_lst:\n",
    "                        try:\n",
    "                            root_candidate = parse[:parse.index('+')]\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                        root_set.add(root_candidate.lower())\n",
    "                    if root_set:\n",
    "                        sentence.append(list(root_set))\n",
    "                    \n",
    "                        correct = candidate_lst[0][:candidate_lst[0].index('+')]\n",
    "                        sentence_correct.append(correct.lower())\n",
    "                        \n",
    "                \n",
    "            else:\n",
    "                if line.startswith(start_tag):\n",
    "                    start_parsing = True\n",
    "                    sentence = []\n",
    "                    sentence_correct = []\n",
    "                    \n",
    "        return sentence_correct_lst, sentence_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_filename = 'dataset/train.merge'\n",
    "train_sentence_correct_lst, train_sentence_lst = import_dataset(train_dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    Dense(100, input_shape=(400,)),\n",
    "    Activation('relu'),\n",
    "    Dense(40),\n",
    "    Activation('relu'),\n",
    "    Dense(40),\n",
    "    Activation('relu'),\n",
    "    Dense(2),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "def generate_samples(sentences):\n",
    "    for sentence in sentences:\n",
    "        for w1, w2 in zip(sentence, sentence[1:]):\n",
    "            itert = itertools.product(w1,w2)\n",
    "            yield next(itert), 1\n",
    "            for others in itert:\n",
    "                yield others, 0\n",
    "samples = list(generate_samples(train_sentence_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_subsamples():\n",
    "    subsample_size = 10000\n",
    "    for subsample in range(0, len(samples), subsample_size):\n",
    "        subsamples = samples[subsample: subsample + subsample_size]\n",
    "        train_data = np.array([np.append(word2vec(w1), word2vec(w2)) for (w1,w2),_ in subsamples])\n",
    "        train_labels = np.array([[v==0, v==1] for _, v in subsamples])\n",
    "        print(subsample, len(samples))\n",
    "        yield train_data, train_labels\n",
    "#train_data, train_labels = next(gen_subsamples())\n",
    "#model.fit(train_data, train_labels, epochs=10, batch_size=32)\n",
    "for train_data, train_labels in gen_subsamples():\n",
    "    model.fit(train_data, train_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"dense_100_40_40_2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"dense_100_40_40_2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([np.append(word2vec(\"new\"), word2vec(\"york\"))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = 'dataset/test.merge'\n",
    "sentence_correct_lst, sentence_lst = import_dataset(dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreModel:\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def predict(self, sentence):\n",
    "        max_score = float('-inf')\n",
    "        predict_sentence = None\n",
    "        self._cache = {}\n",
    "        for element in itertools.product(*sentence):\n",
    "            score = self.calc_sentence_score(element)\n",
    "            if self.verbose:\n",
    "                print(element)\n",
    "                print('Score: %.2f' % score)\n",
    "                print()\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                predict_sentence = list(element)\n",
    "        return predict_sentence\n",
    "    \n",
    "    def pair_score(self, word1, word2):\n",
    "        if (word1, word2) in self._cache:\n",
    "            return self._cache[(word1, word2)]\n",
    "        try:\n",
    "            vec1 = M[word2index[word1]]\n",
    "            vec2 = M[word2index[word2]]\n",
    "        except:\n",
    "            return 0\n",
    "        #return np.abs(np.dot(vec1, vec2))\n",
    "        self._cache[(word1, word2)] = model.predict(np.array([np.append(vec1, vec2)]))[0][1]\n",
    "        return self._cache[(word1, word2)]\n",
    "\n",
    "    def calc_sentence_score(self, sentence):\n",
    "        score = 0\n",
    "\n",
    "        if len(sentence) <= 1:\n",
    "            return score\n",
    "\n",
    "        for i in range(len(sentence) - 1):\n",
    "            word1 = sentence[i]\n",
    "            word2 = sentence[i + 1]\n",
    "            #score += self.pair_score(word1, word2)\n",
    "            score += np.log(self.pair_score(word1, word2) + 1)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words : 861\n",
      "Correctly predicted : 810\n",
      "Word Accuracy : 0.941\n",
      "Total number of sentences : 42\n",
      "Correctly predicted : 18\n",
      "Sentence Accuracy : 0.429\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "false_count = 0\n",
    "\n",
    "false_sentence_count = 0\n",
    "\n",
    "mdl = ScoreModel()\n",
    "for num, (sentence, sentence_correct) in enumerate(zip(sentence_lst, sentence_correct_lst)):\n",
    "    predict_sentence = mdl.predict(sentence)\n",
    "    has_false_word = False\n",
    "    for word1, word2 in zip(predict_sentence, sentence_correct):\n",
    "        if word1 == word2:\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            false_count += 1\n",
    "            if not has_false_word:\n",
    "              has_false_word = True\n",
    "              false_sentence_count += 1\n",
    "\n",
    "total_count = correct_count + false_count\n",
    "accuracy = correct_count / total_count\n",
    "\n",
    "total_sentence_count = len(sentence_correct_lst)\n",
    "correct_sentence_count = total_sentence_count - false_sentence_count\n",
    "sentence_accuracy = correct_sentence_count / total_sentence_count\n",
    "\n",
    "print('Total number of words : %s' % total_count)\n",
    "print('Correctly predicted : %s' % correct_count)\n",
    "print('Word Accuracy : %.3f' % accuracy)\n",
    "\n",
    "print('Total number of sentences : %s' % total_sentence_count)\n",
    "print('Correctly predicted : %s' % correct_sentence_count)\n",
    "print('Sentence Accuracy : %.3f' % sentence_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   Evaluation \\ Model   | Zemberek | VecSim | 2 Hidden Layer Neural Network | 3 Hidden Layer Neural Network |\n",
    "|------------------------|----------|--------|-------------------------------|-------------------------------|\n",
    "| Word Accuracy          | 0.931    | 0.938  | 0.934                         | 0.938                         |\n",
    "| # of Correct Words     | 803      | 808    | 804                           | 808                           |\n",
    "| Sentence Accuracy      | 0.380    | 0.333  | 0.381                         | 0.381                         |\n",
    "| # of Correct Sentences | 16       | 14     | 16                            | 16                            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence):\n",
    "    sentence = '\\'' + sentence + '\\''\n",
    "    word_lst = !./trnltk/parser.py {sentence}\n",
    "    #print(word_lst)\n",
    "    retval = []\n",
    "    for word in word_lst:\n",
    "        root_lst = word.split()\n",
    "        retval.append(root_lst)\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    yüklenen yükle+Verb^DB+Verb+Pass+Pos^DB+Adj+PresPart yük+Noun+A3sg+Pnon+Nom^DB+Verb+Acquire+Pos^DB+Adj+PresPart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tSentence Examples\n",
    "\t\n",
    "\tdolar fiyatları beş TL seviyesinde bulunurken Euro fiyatları altı TL seviyesinde hareket ediyor\n",
    "\tyorulunca alın damarları gözükmeye başladı\n",
    "\tistediğiniz kadar ürün alın \n",
    "\tsözlerine çok alındı (Wrong)\n",
    "\tya iyi olarak ölürsün ya da kötüye dönüşecek kadar uzun yaşarsın\n",
    "\tnedir amacımız bunu göndermekle uzaylılara karsı bir sinerji yaratalım dostluk olsun mu\n",
    "\tkafanızı kullansaydınız o taşların doğada bulunan 4 elementi simgelediğini anlardınız\n",
    "\t\n",
    "\tzorluklar karşısında hemen yılan başarılı olamaz\n",
    "\tormandaki uzun yılan yavaşça avına doğru hareket ediyordu\n",
    "\tormandaki uzun zehirli yılan yavaşça avına doğru hareket ediyordu\n",
    "\t\n",
    "\tne mutlu türküm diyene\n",
    "\tkedi kafasının geçtiği her aralıktan geçebilir\n",
    "\taralık ayında günler kısalır\n",
    "\t\n",
    "\tuzay gemisinin kalkanlarını etkinleştirdi\n",
    "\tyerinden kalkanları öğretmen uyardı\n",
    "    \n",
    "    dolar kuru iyice yükseldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of possibilities : 4\n",
      "[['orman'], ['uzun', 'uz'], ['zehir'], ['yıl', 'yılan'], ['yavaş'], ['av'], ['doğru'], ['hareket'], ['et']]\n",
      "ormandaki uzun zehirli yılan yavaşça avına doğru hareket ediyordu\n",
      "('orman', 'uzun', 'zehir', 'yıl', 'yavaş', 'av', 'doğru', 'hareket', 'et')\n",
      "Score: 3.57\n",
      "\n",
      "('orman', 'uzun', 'zehir', 'yılan', 'yavaş', 'av', 'doğru', 'hareket', 'et')\n",
      "Score: 3.09\n",
      "\n",
      "('orman', 'uz', 'zehir', 'yıl', 'yavaş', 'av', 'doğru', 'hareket', 'et')\n",
      "Score: 3.57\n",
      "\n",
      "('orman', 'uz', 'zehir', 'yılan', 'yavaş', 'av', 'doğru', 'hareket', 'et')\n",
      "Score: 3.09\n",
      "\n",
      "-------\n",
      "Predicted roots : \n",
      "['orman', 'uzun', 'zehir', 'yıl', 'yavaş', 'av', 'doğru', 'hareket', 'et']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'ormandaki uzun zehirli yılan yavaşça avına doğru hareket ediyordu'\n",
    "parsed_sentence = parse_sentence(sentence)\n",
    "\n",
    "combination_count = 1\n",
    "for word in parsed_sentence:\n",
    "  combination_count *= len(word)\n",
    "print('Total number of possibilities : %s' % combination_count)\n",
    "\n",
    "print(parsed_sentence)\n",
    "print(sentence)\n",
    "\n",
    "vmodel = ScoreModel(verbose=True)\n",
    "prediction = vmodel.predict(parsed_sentence)\n",
    "\n",
    "print('-------')\n",
    "print('Predicted roots : ')\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
