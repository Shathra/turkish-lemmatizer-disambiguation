{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import collections\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = np.load('word2vec.npy')\n",
    "word2index = dict()\n",
    "with open('word_lst.txt') as file:\n",
    "    for counter, line in enumerate(file):\n",
    "        word = line.strip()\n",
    "        word2index[word] = counter\n",
    "\n",
    "def word2vec(word):\n",
    "    idx = word2index.get(word)\n",
    "    if idx is None:\n",
    "        return np.zeros(200)\n",
    "    return M[idx]\n",
    "\n",
    "        \n",
    "def get_vec(word):\n",
    "    try:\n",
    "        retval = M[word2index[word]]\n",
    "    except KeyError:\n",
    "        retval = np.zeros(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_dataset(filename):\n",
    "    \n",
    "    start_tag = '<S>'\n",
    "    end_tag = '</S>'\n",
    "    \n",
    "    sentence_lst = [] # Sentence is a list word which is list of candidate roots\n",
    "    sentence_correct_lst = []\n",
    "    with open(filename) as file:\n",
    "        start_parsing = False\n",
    "        for line in file:\n",
    "            if start_parsing:\n",
    "                if line.startswith(end_tag):\n",
    "                    start_parsing = False\n",
    "                    sentence_lst.append(sentence)\n",
    "                    sentence_correct_lst.append(sentence_correct)\n",
    "                else:\n",
    "                    root_set = set()\n",
    "                    candidate_lst = line.split()[1:]\n",
    "                    for parse in candidate_lst:\n",
    "                        try:\n",
    "                            root_candidate = parse[:parse.index('+')]\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "                        root_set.add(root_candidate.lower())\n",
    "                    if root_set:\n",
    "                        sentence.append(list(root_set))\n",
    "                    \n",
    "                        correct = candidate_lst[0][:candidate_lst[0].index('+')]\n",
    "                        sentence_correct.append(correct.lower())\n",
    "                        \n",
    "                \n",
    "            else:\n",
    "                if line.startswith(start_tag):\n",
    "                    start_parsing = True\n",
    "                    sentence = []\n",
    "                    sentence_correct = []\n",
    "                    \n",
    "        return sentence_correct_lst, sentence_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_filename = 'dataset/train.merge'\n",
    "train_sentence_correct_lst, train_sentence_lst = import_dataset(train_dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    Dense(100, input_shape=(400,)),\n",
    "    Activation('relu'),\n",
    "    Dense(40),\n",
    "    Activation('relu'),\n",
    "    Dense(40),\n",
    "    Activation('relu'),\n",
    "    Dense(2),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "def generate_samples(sentences):\n",
    "    for sentence in sentences:\n",
    "        for w1, w2 in zip(sentence, sentence[1:]):\n",
    "            itert = itertools.product(w1,w2)\n",
    "            yield next(itert), 1\n",
    "            for others in itert:\n",
    "                yield others, 0\n",
    "samples = list(generate_samples(train_sentence_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_subsamples():\n",
    "    subsample_size = 10000\n",
    "    for subsample in range(0, len(samples), subsample_size):\n",
    "        subsamples = samples[subsample: subsample + subsample_size]\n",
    "        train_data = np.array([np.append(word2vec(w1), word2vec(w2)) for (w1,w2),_ in subsamples])\n",
    "        train_labels = np.array([[v==0, v==1] for _, v in subsamples])\n",
    "        print(subsample, len(samples))\n",
    "        yield train_data, train_labels\n",
    "#train_data, train_labels = next(gen_subsamples())\n",
    "#model.fit(train_data, train_labels, epochs=10, batch_size=32)\n",
    "for train_data, train_labels in gen_subsamples():\n",
    "    model.fit(train_data, train_labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"dense_100_40_40_2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"dense_100_40_40_2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([np.append(word2vec(\"new\"), word2vec(\"york\"))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename = 'dataset/test.merge'\n",
    "sentence_correct_lst, sentence_lst = import_dataset(dataset_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreModel:\n",
    "    def __init__(self, verbose=False):\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def predict(self, sentence):\n",
    "        max_score = float('-inf')\n",
    "        predict_sentence = None\n",
    "        self._cache = {}\n",
    "        for element in itertools.product(*sentence):\n",
    "            score = self.calc_sentence_score(element)\n",
    "            if self.verbose:\n",
    "                print(element)\n",
    "                print('Score: %.2f' % score)\n",
    "                print()\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                predict_sentence = list(element)\n",
    "        return predict_sentence\n",
    "    \n",
    "    def pair_score(self, word1, word2):\n",
    "        if (word1, word2) in self._cache:\n",
    "            return self._cache[(word1, word2)]\n",
    "        try:\n",
    "            vec1 = M[word2index[word1]]\n",
    "            vec2 = M[word2index[word2]]\n",
    "        except:\n",
    "            return 0\n",
    "        #return np.abs(np.dot(vec1, vec2))\n",
    "        self._cache[(word1, word2)] = model.predict(np.array([np.append(vec1, vec2)]))[0][1]\n",
    "        return self._cache[(word1, word2)]\n",
    "\n",
    "    def calc_sentence_score(self, sentence):\n",
    "        score = 0\n",
    "\n",
    "        if len(sentence) <= 1:\n",
    "            return score\n",
    "\n",
    "        for i in range(len(sentence) - 1):\n",
    "            word1 = sentence[i]\n",
    "            word2 = sentence[i + 1]\n",
    "            score += self.pair_score(word1, word2)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words : 861\n",
      "Correctly predicted : 813\n",
      "Accuracy : 0.944\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0\n",
    "false_count = 0\n",
    "\n",
    "mdl = ScoreModel()\n",
    "for num, (sentence, sentence_correct) in enumerate(zip(sentence_lst, sentence_correct_lst)):\n",
    "    predict_sentence = mdl.predict(sentence)\n",
    "    for word1, word2 in zip(predict_sentence, sentence_correct):\n",
    "        if word1 == word2:\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            false_count += 1\n",
    "\n",
    "total_count = correct_count + false_count\n",
    "accuracy = correct_count / total_count\n",
    "\n",
    "print('Total number of words : %s' % total_count)\n",
    "print('Correctly predicted : %s' % correct_count)\n",
    "print('Accuracy : %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence):\n",
    "    sentence = '\\'' + sentence + '\\''\n",
    "    word_lst = !./trnltk/parser.py {sentence}\n",
    "    #print(word_lst)\n",
    "    retval = []\n",
    "    for word in word_lst:\n",
    "        root_lst = word.split()\n",
    "        retval.append(root_lst)\n",
    "    return retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    yüklenen yükle+Verb^DB+Verb+Pass+Pos^DB+Adj+PresPart yük+Noun+A3sg+Pnon+Nom^DB+Verb+Acquire+Pos^DB+Adj+PresPart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Sentence Examples\n",
    "    \n",
    "    dolar fiyatları beş TL seviyesinde bulunurken Euro fiyatları altı TL seviyesinde hareket ediyor\n",
    "    yorulunca alın damarları gözükmeye başladı\n",
    "    istediğiniz kadar ürün alın \n",
    "    sözlerine çok alındı (Wrong)\n",
    "    ya iyi olarak ölürsün ya da kötüye dönüşecek kadar uzun yaşarsın\n",
    "    nedir amacımız bunu göndermekle uzaylılara karsı bir sinerji yaratalım dostluk olsun mu\n",
    "    kafanızı kullansaydınız o taşların doğada bulunan 4 elementi simgelediğini anlardınız"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sentence = 'istediğiniz kadar ürün alın'\n",
    "parsed_sentence = parse_sentence(sentence)\n",
    "\n",
    "combination_count = 1\n",
    "for word in parsed_sentence:\n",
    "  combination_count *= len(word)\n",
    "print('Total number of possibilities : %s' % combination_count)\n",
    "\n",
    "print(parsed_sentence)\n",
    "print(sentence)\n",
    "\n",
    "vmodel = ScoreModel(verbose=True)\n",
    "prediction = vmodel.predict(parsed_sentence)\n",
    "\n",
    "print('-------')\n",
    "print('Predicted roots : ')\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
